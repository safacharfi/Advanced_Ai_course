{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The following additional libraries are needed to run this\nnotebook. Note that running on Colab is experimental, please report a Github\nissue if you have any problem.","metadata":{"id":"1d625b67"}},{"cell_type":"code","source":"!pip install d2l==0.17.0","metadata":{"id":"a526d0da","execution":{"iopub.status.busy":"2024-11-13T19:02:49.209663Z","iopub.execute_input":"2024-11-13T19:02:49.210589Z","iopub.status.idle":"2024-11-13T19:03:02.272688Z","shell.execute_reply.started":"2024-11-13T19:02:49.210545Z","shell.execute_reply":"2024-11-13T19:03:02.271480Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow==2.8.0","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:03:02.275114Z","iopub.execute_input":"2024-11-13T19:03:02.275492Z","iopub.status.idle":"2024-11-13T19:03:32.974546Z","shell.execute_reply.started":"2024-11-13T19:03:02.275455Z","shell.execute_reply":"2024-11-13T19:03:32.973492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Long Short-Term Memory (LSTM)\n:label:`sec_lstm`\n\nThe challenge to address long-term information preservation and short-term input\nskipping in latent variable models has existed for a long time. One of the\nearliest approaches to address this was the\nlong short-term memory (LSTM) :cite:`Hochreiter.Schmidhuber.1997`. It shares many of the properties of the\nGRU.\nInterestingly, LSTMs have a slightly more complex\ndesign than GRUs but predates GRUs by almost two decades.\n\n\n\n## Gated Memory Cell\n\nArguably LSTM's design is inspired\nby logic gates of a computer.\nLSTM introduces a *memory cell* (or *cell* for short)\nthat has the same shape as the hidden state\n(some literatures consider the memory cell\nas a special type of the hidden state),\nengineered to record additional information.\nTo control the memory cell\nwe need a number of gates.\nOne gate is needed to read out the entries from the\ncell.\nWe will refer to this as the\n*output gate*.\nA second gate is needed to decide when to read data into the\ncell.\nWe refer to this as the *input gate*.\nLast, we need a mechanism to reset\nthe content of the cell, governed by a *forget gate*.\nThe motivation for such a\ndesign is the same as that of GRUs,\nnamely to be able to decide when to remember and\nwhen to ignore inputs in the hidden state via a dedicated mechanism. Let us see\nhow this works in practice.\n\n\n### Input Gate, Forget Gate, and Output Gate\n\nJust like in GRUs,\nthe data feeding into the LSTM gates are\nthe input at the current time step and\nthe hidden state of the previous time step,\nas illustrated in :numref:`lstm_0`.\nThey are processed by\nthree fully-connected layers with a sigmoid activation function to compute the values of\nthe input, forget. and output gates.\nAs a result, values of the three gates\nare in the range of $(0, 1)$.\n\n![Computing the input gate, the forget gate, and the output gate in an LSTM model.](https://github.com/d2l-ai/d2l-tensorflow-colab-classic/blob/master/img/lstm-0.svg?raw=1)\n:label:`lstm_0`\n\nMathematically,\nsuppose that there are $h$ hidden units, the batch size is $n$, and the number of inputs is $d$.\nThus, the input is $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ and the hidden state of the previous time step is $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$. Correspondingly, the gates at time step $t$\nare defined as follows: the input gate is $\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$, the forget gate is $\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$, and the output gate is $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$. They are calculated as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}_{xi}, \\mathbf{W}_{xf}, \\mathbf{W}_{xo} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{hi}, \\mathbf{W}_{hf}, \\mathbf{W}_{ho} \\in \\mathbb{R}^{h \\times h}$ are weight parameters and $\\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}$ are bias parameters.\n\n### Candidate Memory Cell\n\nNext we design the memory cell. Since we have not specified the action of the various gates yet, we first introduce the *candidate* memory cell $\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$. Its computation is similar to that of the three gates described above, but using a $\\tanh$ function with a value range for $(-1, 1)$ as the activation function. This leads to the following equation at time step $t$:\n\n$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),$$\n\nwhere $\\mathbf{W}_{xc} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{hc} \\in \\mathbb{R}^{h \\times h}$ are weight parameters and $\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}$ is a bias parameter.\n\nA quick illustration of the candidate memory cell is shown in :numref:`lstm_1`.\n\n![Computing the candidate memory cell in an LSTM model.](https://github.com/d2l-ai/d2l-tensorflow-colab-classic/blob/master/img/lstm-1.svg?raw=1)\n:label:`lstm_1`\n\n### Memory Cell\n\nIn GRUs, we have a mechanism to govern input and forgetting (or skipping).\nSimilarly,\nin LSTMs we have two dedicated gates for such purposes: the input gate $\\mathbf{I}_t$ governs how much we take new data into account via $\\tilde{\\mathbf{C}}_t$ and the forget gate $\\mathbf{F}_t$ addresses how much of the old memory cell content $\\mathbf{C}_{t-1} \\in \\mathbb{R}^{n \\times h}$ we retain. Using the same pointwise multiplication trick as before, we arrive at the following update equation:\n\n$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$\n\nIf the forget gate is always approximately 1 and the input gate is always approximately 0, the past memory cells $\\mathbf{C}_{t-1}$ will be saved over time and passed to the current time step.\nThis design is introduced to alleviate the vanishing gradient problem and to better capture\nlong range dependencies within sequences.\n\nWe thus arrive at the flow diagram in :numref:`lstm_2`.\n\n![Computing the memory cell in an LSTM model.](https://github.com/d2l-ai/d2l-tensorflow-colab-classic/blob/master/img/lstm-2.svg?raw=1)\n\n:label:`lstm_2`\n\n\n### Hidden State\n\nLast, we need to define how to compute the hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$. This is where the output gate comes into play. In LSTM it is simply a gated version of the $\\tanh$ of the memory cell.\nThis ensures that the values of $\\mathbf{H}_t$ are always in the interval $(-1, 1)$.\n\n$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$\n\n\nWhenever the output gate approximates 1 we effectively pass all memory information through to the predictor, whereas for the output gate close to 0 we retain all the information only within the memory cell and perform no further processing.\n\n\n\n:numref:`lstm_3` has a graphical illustration of the data flow.\n\n![Computing the hidden state in an LSTM model.](https://github.com/d2l-ai/d2l-tensorflow-colab-classic/blob/master/img/lstm-3.svg?raw=1)\n:label:`lstm_3`\n\n\n\n## Implementation from Scratch\n\nNow let us implement an LSTM from scratch.\nAs same as the experiments in :numref:`sec_rnn_scratch`,\nwe first load the time machine dataset.\n","metadata":{"origin_pos":0,"id":"b6b5e8a4"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom d2l import tensorflow as d2l\n\nbatch_size, num_steps = 32, 35\ntrain_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)","metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"b517f940","outputId":"878058f6-e779-4031-a26e-4fcfa96aa7ff","execution":{"iopub.status.busy":"2024-11-13T19:03:32.976531Z","iopub.execute_input":"2024-11-13T19:03:32.977159Z","iopub.status.idle":"2024-11-13T19:03:37.302228Z","shell.execute_reply.started":"2024-11-13T19:03:32.977109Z","shell.execute_reply":"2024-11-13T19:03:37.301148Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [**Initializing Model Parameters**]\n\nNext we need to define and initialize the model parameters. As previously, the hyperparameter `num_hiddens` defines the number of hidden units. We initialize weights following a Gaussian distribution with 0.01 standard deviation, and we set the biases to 0.\n","metadata":{"origin_pos":4,"id":"a8d1e772"}},{"cell_type":"code","source":"def get_lstm_params(vocab_size, num_hiddens):\n    num_inputs = num_outputs = vocab_size\n\n    def normal(shape):\n        return tf.Variable(tf.random.normal(shape=shape, stddev=0.01,\n                                            mean=0, dtype=tf.float32))\n    def three():\n        return (normal((num_inputs, num_hiddens)),\n                normal((num_hiddens, num_hiddens)),\n                tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32))\n\n    W_xi, W_hi, b_i = three()  # Input gate parameters\n    W_xf, W_hf, b_f = three()  # Forget gate parameters\n    W_xo, W_ho, b_o = three()  # Output gate parameters\n    W_xc, W_hc, b_c = three()  # Candidate memory cell parameters\n    # Output layer parameters\n    W_hq = normal((num_hiddens, num_outputs))\n    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n    # Attach gradients\n    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n              b_c, W_hq, b_q]\n    return params","metadata":{"origin_pos":7,"tab":["tensorflow"],"id":"d264f3e9","execution":{"iopub.status.busy":"2024-11-13T19:03:37.304681Z","iopub.execute_input":"2024-11-13T19:03:37.305248Z","iopub.status.idle":"2024-11-13T19:03:37.315219Z","shell.execute_reply.started":"2024-11-13T19:03:37.305210Z","shell.execute_reply":"2024-11-13T19:03:37.314111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining the Model\n\nIn [**the initialization function**], the hidden state of the LSTM needs to return an *additional* memory cell with a value of 0 and a shape of (batch size, number of hidden units). Hence we get the following state initialization.\n","metadata":{"origin_pos":8,"id":"a297d281"}},{"cell_type":"code","source":"def init_lstm_state(batch_size, num_hiddens):\n    return (tf.zeros(shape=(batch_size, num_hiddens)),\n            tf.zeros(shape=(batch_size, num_hiddens)))","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"4dbc6a1c","execution":{"iopub.status.busy":"2024-11-13T19:03:37.316942Z","iopub.execute_input":"2024-11-13T19:03:37.317360Z","iopub.status.idle":"2024-11-13T19:03:40.701320Z","shell.execute_reply.started":"2024-11-13T19:03:37.317314Z","shell.execute_reply":"2024-11-13T19:03:40.700236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"[**The actual model**] is defined just like what we discussed before: providing three gates and an auxiliary memory cell. Note that only the hidden state is passed to the output layer. The memory cell $\\mathbf{C}_t$ does not directly participate in the output computation.\n","metadata":{"origin_pos":12,"id":"7239dc48"}},{"cell_type":"code","source":"def lstm(inputs, state, params):\n    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params\n    (H, C) = state\n    outputs = []\n    for X in inputs:\n        X=tf.reshape(X,[-1,W_xi.shape[0]])\n        I = tf.sigmoid(tf.matmul(X, W_xi) + tf.matmul(H, W_hi) + b_i)\n        F = tf.sigmoid(tf.matmul(X, W_xf) + tf.matmul(H, W_hf) + b_f)\n        O = tf.sigmoid(tf.matmul(X, W_xo) + tf.matmul(H, W_ho) + b_o)\n        C_tilda = tf.tanh(tf.matmul(X, W_xc) + tf.matmul(H, W_hc) + b_c)\n        C = F * C + I * C_tilda\n        H = O * tf.tanh(C)\n        Y = tf.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return tf.concat(outputs, axis=0), (H,C)","metadata":{"origin_pos":15,"tab":["tensorflow"],"id":"b730a9fa","execution":{"iopub.status.busy":"2024-11-13T19:03:40.702697Z","iopub.execute_input":"2024-11-13T19:03:40.703025Z","iopub.status.idle":"2024-11-13T19:03:40.711801Z","shell.execute_reply.started":"2024-11-13T19:03:40.702992Z","shell.execute_reply":"2024-11-13T19:03:40.710851Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [**Training**] and Prediction\n\nLet us train an LSTM as same as what we did in :numref:`sec_gru`, by instantiating the `RNNModelScratch` class as introduced in :numref:`sec_rnn_scratch`.\n","metadata":{"origin_pos":16,"id":"eb0080df"}},{"cell_type":"code","source":"vocab_size, num_hiddens, device_name = len(vocab), 256, d2l.try_gpu()._device_name\nnum_epochs, lr = 500, 1\nstrategy = tf.distribute.OneDeviceStrategy(device_name)\nwith strategy.scope():\n    model = d2l.RNNModelScratch(len(vocab), num_hiddens, init_lstm_state, lstm, get_lstm_params)\nd2l.train_ch8(model, train_iter, vocab, lr, num_epochs, strategy)","metadata":{"origin_pos":18,"tab":["tensorflow"],"id":"09f763cb","outputId":"f1e5c396-427b-4b5b-e1f6-77cb02350b76","execution":{"iopub.status.busy":"2024-11-13T19:03:40.712981Z","iopub.execute_input":"2024-11-13T19:03:40.713347Z","iopub.status.idle":"2024-11-13T19:24:43.501005Z","shell.execute_reply.started":"2024-11-13T19:03:40.713305Z","shell.execute_reply":"2024-11-13T19:24:43.499723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## [**Concise Implementation**]\n\nUsing high-level APIs,\nwe can directly instantiate an `LSTM` model.\nThis encapsulates all the configuration details that we made explicit above. The code is significantly faster as it uses compiled operators rather than Python for many details that we spelled out in detail before.\n","metadata":{"origin_pos":19,"id":"a6180585"}},{"cell_type":"code","source":"lstm_cell = tf.keras.layers.LSTMCell(num_hiddens,\n    kernel_initializer='glorot_uniform')\nlstm_layer = tf.keras.layers.RNN(lstm_cell, time_major=True,\n    return_sequences=True, return_state=True)\ndevice_name = d2l.try_gpu()._device_name\nstrategy = tf.distribute.OneDeviceStrategy(device_name)\nwith strategy.scope():\n    model = d2l.RNNModel(lstm_layer, vocab_size=len(vocab))\nd2l.train_ch8(model, train_iter, vocab, lr, num_epochs, strategy)","metadata":{"origin_pos":22,"tab":["tensorflow"],"id":"15dca75b","outputId":"d6794d74-bbc7-4609-d65a-029cb6c95051","execution":{"iopub.status.busy":"2024-11-13T19:24:43.503115Z","iopub.execute_input":"2024-11-13T19:24:43.504163Z","iopub.status.idle":"2024-11-13T19:36:05.982231Z","shell.execute_reply.started":"2024-11-13T19:24:43.504089Z","shell.execute_reply":"2024-11-13T19:36:05.980315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LSTMs are the prototypical latent variable autoregressive model with nontrivial state control.\nMany variants thereof have been proposed over the years, e.g., multiple layers, residual connections, different types of regularization. However, training LSTMs and other sequence models (such as GRUs) are quite costly due to the long range dependency of the sequence.\nLater we will encounter alternative models such as transformers that can be used in some cases.\n\n\n## Summary\n\n* LSTMs have three types of gates: input gates, forget gates, and output gates that control the flow of information.\n* The hidden layer output of LSTM includes the hidden state and the memory cell. Only the hidden state is passed into the output layer. The memory cell is entirely internal.\n* LSTMs can alleviate vanishing and exploding gradients.\n\n\n## Exercises\n\n1. Adjust the hyperparameters and analyze the their influence on running time, perplexity, and the output sequence.\n1. How would you need to change the model to generate proper words as opposed to sequences of characters?\n1. Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension. Pay special attention to the training and inference cost.\n1. Since the candidate memory cell ensures that the value range is between $-1$ and $1$ by  using the $\\tanh$ function, why does the hidden state need to use the $\\tanh$ function again to ensure that the output value range is between $-1$ and $1$?\n1. Implement an LSTM model for time series prediction rather than character sequence prediction.\n","metadata":{"origin_pos":23,"id":"82ce1d2f"}},{"cell_type":"markdown","source":"## Exercise 1: Adjust the hyperparameters and analyze their influence on running time, perplexity, and the output sequence.","metadata":{}},{"cell_type":"code","source":"# Adjust hyperparameters\nnum_hiddens = 128  # Number of hidden units\nnum_epochs = 300   # Number of epochs\nlr = 0.5           # Learning rate\n\n# Train the model with new hyperparameters\nstrategy = tf.distribute.OneDeviceStrategy(device_name)\nwith strategy.scope():\n    model = d2l.RNNModelScratch(len(vocab), num_hiddens, init_lstm_state, lstm, get_lstm_params)\nd2l.train_ch8(model, train_iter, vocab, lr, num_epochs, strategy)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:36:05.984066Z","iopub.execute_input":"2024-11-13T19:36:05.985235Z","iopub.status.idle":"2024-11-13T19:44:54.936053Z","shell.execute_reply.started":"2024-11-13T19:36:05.985172Z","shell.execute_reply":"2024-11-13T19:44:54.934938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 2: Change the model to generate proper words as opposed to sequences of characters.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\n# Example text data\ntext = \"This is an example text for the LSTM model. This is another sentence.\"\n\n# Tokenize the text into words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([text])\nsequences = tokenizer.texts_to_sequences([text])\nword_index = tokenizer.word_index\n\n# Prepare the data\nvocab_size = len(word_index) + 1\nmax_length = max(len(seq) for seq in sequences)\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n# Split the data into input and target sequences\nX = padded_sequences[:, :-1]\ny = padded_sequences[:, 1:]\n\n# Build the LSTM model using tf.keras\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=256, input_length=max_length-1),\n    tf.keras.layers.LSTM(256, return_sequences=True),\n    tf.keras.layers.Dense(vocab_size, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\n# Train the model\nmodel.fit(X, y, epochs=500, verbose=2)\n\n# Function to generate text\ndef generate_text(model, tokenizer, seed_text, num_words):\n    for _ in range(num_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_length-1, padding='post')\n        predicted = np.argmax(model.predict(token_list), axis=-1)\n        output_word = tokenizer.index_word[int(predicted[0][-1])]\n        seed_text += \" \" + output_word\n    return seed_text\n\n# Generate text\nprint(generate_text(model, tokenizer, \"This is\", 10))","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:56:12.417077Z","iopub.execute_input":"2024-11-13T19:56:12.417868Z","iopub.status.idle":"2024-11-13T19:56:22.260018Z","shell.execute_reply.started":"2024-11-13T19:56:12.417821Z","shell.execute_reply":"2024-11-13T19:56:22.259009Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 3: Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport time\n\n# Define the hidden dimension and input size\nhidden_dim = 256\ninput_dim = 256\nbatch_size = 32\ntime_steps = 35\n\n# Generate random data for testing\ninputs = tf.random.normal((batch_size, time_steps, input_dim))\n\n# Function to measure time taken by a model\ndef measure_model_runtime(model, inputs, iterations=100):\n    start_time = time.time()\n    for _ in range(iterations):\n        _ = model(inputs)\n    end_time = time.time()\n    avg_time = (end_time - start_time) / iterations\n    return avg_time\n\n# Define and measure runtime for a regular RNN model\nrnn_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(hidden_dim, return_sequences=True)\n])\nrnn_time = measure_model_runtime(rnn_model, inputs)\nprint(f\"Average computation time per iteration (RNN): {rnn_time:.5f} seconds\")\n\n# Define and measure runtime for a GRU model\ngru_model = tf.keras.Sequential([\n    tf.keras.layers.GRU(hidden_dim, return_sequences=True)\n])\ngru_time = measure_model_runtime(gru_model, inputs)\nprint(f\"Average computation time per iteration (GRU): {gru_time:.5f} seconds\")\n\n# Define and measure runtime for an LSTM model\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.LSTM(hidden_dim, return_sequences=True)\n])\nlstm_time = measure_model_runtime(lstm_model, inputs)\nprint(f\"Average computation time per iteration (LSTM): {lstm_time:.5f} seconds\")\n\n# Compare results\nprint(\"\\nComparison of computational costs:\")\nprint(f\"RNN time: {rnn_time:.5f} seconds\")\nprint(f\"GRU time: {gru_time:.5f} seconds\")\nprint(f\"LSTM time: {lstm_time:.5f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:57:03.034666Z","iopub.execute_input":"2024-11-13T19:57:03.035519Z","iopub.status.idle":"2024-11-13T19:57:21.531768Z","shell.execute_reply.started":"2024-11-13T19:57:03.035467Z","shell.execute_reply":"2024-11-13T19:57:21.530828Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 4: Why does the hidden state need to use the $\\tanh$ function again to ensure that the output value range is between $-1$ and $1$?","metadata":{}},{"cell_type":"markdown","source":"In RNNs, GRUs, and LSTMs, the hidden state often passes through a `tanh` activation function to ensure that its values lie within the range \\([-1, 1]\\). This is done for several important reasons:\n\n### 1. **Control of Hidden State Values**\n   - The `tanh` function outputs values between \\(-1\\) and \\(1\\), which prevents the hidden states from growing too large. This is important because unbounded hidden states could lead to instability and potential issues with exploding gradients, especially in longer sequences.\n   - By keeping the hidden state within a limited range, `tanh` helps maintain numerical stability and prevents the model from producing extremely high or low values that could disrupt learning.\n\n### 2. **Non-linearity and Signal Modulation**\n   - The `tanh` function introduces non-linearity into the model. This non-linearity allows the network to learn more complex patterns and dependencies in the data, as linear transformations alone cannot capture intricate relationships.\n   - The bounded output of `tanh` also acts as a modulator for information flow. It allows the model to \"forget\" or \"retain\" certain aspects of previous information by controlling how much of the previous hidden state influences the current hidden state. This improves the model’s ability to handle long-range dependencies.\n\n### 3. **Gradient Stability**\n   - `tanh` helps mitigate the vanishing gradient problem. The bounded nature of the `tanh` function reduces the risk of large gradients, which could destabilize the training process. Moreover, by keeping the gradients in a moderate range, `tanh` facilitates more stable weight updates and promotes efficient learning.\n\n### 4. **Consistency Across Layers**\n   - In networks with multiple recurrent layers, using a consistent output range (such as \\([-1, 1]\\) with `tanh`) ensures that each layer’s hidden state is on a comparable scale. This consistency helps maintain signal clarity and promotes efficient learning across layers, improving the model's ability to learn meaningful patterns.\n\nThus, by using the `tanh` function to keep the hidden state within \\([-1, 1]\\), recurrent networks become more stable, learnable, and capable of handling complex sequential data.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 5: Implement an LSTM model for time series prediction rather than character sequence prediction.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# Generate synthetic time series data (e.g., a sine wave)\ndef generate_time_series(batch_size, n_steps):\n    freq1, freq2, offset1, offset2 = np.random.rand(4, batch_size, 1)\n    time = np.linspace(0, 1, n_steps)\n    series = 0.5 * np.sin((time - offset1) * (freq1 * 10 + 10))   # Wave 1\n    series += 0.2 * np.sin((time - offset2) * (freq2 * 20 + 20))  # Wave 2\n    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # Noise\n    return series[..., np.newaxis].astype(np.float32)\n\n# Parameters\nbatch_size = 32\nn_steps = 50\n\n# Generate the data\nseries = generate_time_series(batch_size, n_steps + 1)\nX_train, y_train = series[:, :-1], series[:, -1]\n\n# Visualize a sample of the time series\nplt.plot(X_train[0])\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Value\")\nplt.title(\"Sample Time Series\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:58:27.301804Z","iopub.execute_input":"2024-11-13T19:58:27.302538Z","iopub.status.idle":"2024-11-13T19:58:27.602413Z","shell.execute_reply.started":"2024-11-13T19:58:27.302498Z","shell.execute_reply":"2024-11-13T19:58:27.600849Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# Define LSTM model for time series prediction\nmodel = Sequential([\n    LSTM(50, activation='relu', input_shape=(n_steps, 1)),\n    Dense(1)  # Predict the value at the next time step\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:58:58.802783Z","iopub.execute_input":"2024-11-13T19:58:58.803177Z","iopub.status.idle":"2024-11-13T19:58:58.895918Z","shell.execute_reply.started":"2024-11-13T19:58:58.803140Z","shell.execute_reply":"2024-11-13T19:58:58.895004Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:59:58.610017Z","iopub.execute_input":"2024-11-13T19:59:58.610438Z","iopub.status.idle":"2024-11-13T19:59:59.923132Z","shell.execute_reply.started":"2024-11-13T19:59:58.610376Z","shell.execute_reply":"2024-11-13T19:59:59.922121Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate a new series for testing\nX_test, y_test = generate_time_series(batch_size, n_steps + 1)[:, :-1], generate_time_series(batch_size, n_steps + 1)[:, -1]\n\n# Predict the next value in the sequence\ny_pred = model.predict(X_test)\n\n# Visualize the predictions versus the actual values\nplt.plot(y_test, label=\"Actual\")\nplt.plot(y_pred, label=\"Predicted\")\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.title(\"Time Series Prediction\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-13T20:00:03.475596Z","iopub.execute_input":"2024-11-13T20:00:03.476307Z","iopub.status.idle":"2024-11-13T20:00:03.863791Z","shell.execute_reply.started":"2024-11-13T20:00:03.476266Z","shell.execute_reply":"2024-11-13T20:00:03.858571Z"},"trusted":true},"outputs":[],"execution_count":null}]}